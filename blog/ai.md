# AI is Unsustainable

There's been an insane spike in AI usage over the past years. It's to the point that I don't think I even need to explain that sentence - you, dear reader, know it just as well as I do, because it's *everywhere*.

...though, if you do want some numbers:
- ChatGPT's number of weekly users [doubled from 100 million to 200 million users](https://www.theverge.com/2024/8/29/24231685/openai-chatgpt-200-million-weekly-users) between November of 2023 and August of 2024 (less than a year!).
- ChatGPT released in 2022. In the *two years* since then, there have been at least 4 more LLMs released (Google's [Gemini](https://gemini.google.com), Microsoft's [Copilot](https://copilot.microsoft.com), Anthropic's [Claude](https://claude.ai), Facebook's [LLaMA](https://www.llama.com/), and probably more I've managed to miss).
- By March of 2023, according to Pew Research, nearly [60% of US adults](https://www.pewresearch.org/short-reads/2023/05/24/a-majority-of-americans-have-heard-of-chatgpt-but-few-have-tried-it-themselves/) had heard of ChatGPT (and let's be honest - getting 60% of US adults to know about *anything* is impressive!).

I've even had a teacher *encourage AI usage* on assignments. AI has grown explosively.

> **Note:**
>
> Technically, using "AI" here is a bit of a misnomer. Artificial intelligence is an *incredibly* broad field, and I'm only really talking about one specific aspect of it: Commercial LLMs, the **L**arge **L**anguage **M**odels (like ChatGPT and Gemini) being developed commercially by companies. I'm still going to say AI in this post so it's easy to read, but I wanted to leave this disclaimer to remain accurate.



# The Problem

While it's cool to see a product spreading so rapidly, it's unfortunately a very problematic technology. There are *tons* if problems and questions with AI. Many of these problems aren't ones I'm going to address in this post, because they can get rather philosophical and lengthy - I'm not, for example, going to discuss if this technology is truly intelligent.

Instead, I'm going to focus on more factual and objective problems - specifically, these three problems:

1. AI is damaging to the environment, using frankly absurd amounts of water and power. Its power usage is also causing increases in carbon emissions.
2. Companies are overselling the future of AI. It will not grow to meet their claims.
3. AI is not financially sustainable, nor profitable. It is draining immense sums of money from our economy.



# I don't Care

Yeah, these problems sound large and abstract - too big to really matter to you and me and our daily lives.

For a while, I agreed. I actually thought this tech would just breeze over and we'd move on after a bit. However, as time goes on, AI just keeps being integrated into more and more parts of our lives. So I've changed my mind - I've decided AI is very relevant to me, and to you, as well.

AI is relevant to you because companies are forcing it onto you. iOS, macOS, Android, and Windows are now all introducing (or planning to introduce) AI technology directly onto our smartphones and computers. This technology - which wasn't even relevant until the release of ChatGPT 2 years ago - is now being shoved into everything from [web searches](https://blog.google/products/search/generative-ai-google-search-may-2024/) to [app notifications](https://support.apple.com/guide/iphone/summarize-notifications-reduce-interruptions-iph1fbe7d2b9/ios).

AI is relevant to you because it's affecting the planet we live on and the economy we live in. As I'll show later in this post, AI is devouring water, electricity, and money at a faster rate than any technology we've seen before.

AI is relevant to you because remaining ignorant on the topic allows companies to take advantage of you. For as long as people remain ignorant on the topic, companies will manipulate them by pushing more AI products. Doing so maintains hype, attracts potential customers, and draws in investors - all of which boost their precious profits. This process includes you.

AI is relevant to you. You deserve to know its consequences.



# Environmental Impact

AI is *super* resource-intensive to run. Sure - we've all had apps that take forever to load, computers taking minutes to boot, the videogame that makes your laptop's fans run so hard it seems like it's going to take off like a hovercraft - but this is on a different level. AI requires entire *datacenters* of compute power to run. We have buildings, today, that do nothing except process the requests of AI users across the world.

It may sound like I'm being vague about exactly how much compute power AI needs, and you'd be right. AI companies hide figures like this; we *don't get* to know how many datacenters AI companies use [or even where those datacenters are](https://community.openai.com/t/location-of-openai-servers/559880).

However, there are hints as to how much compute power they need. OpenAI proposed building [multiple 5-gigawatt datacenters](https://arstechnica.com/tech-policy/2024/09/openai-asked-us-to-approve-energy-guzzling-5gw-data-centers-report-says/) across the US (5 gigawatts being roughly equivalent to 5 nuclear reactors' worth of energy). They've also stated in their blog that they planned on using [thousands to tens of thousands](https://openai.com/index/openai-and-microsoft/) of datacenter computers owned by Microsoft for their programs. That blog post is from all the way back in 2016 - six years before ChatGPT was released - and computing costs have almost certainly risen drastically since then. In a much newer blogpost announcing the release of GPT-4, OpenAI briefly mentions [building a literal supercomputer](https://openai.com/index/gpt-4-research/) to run the model.

So, while we don't know exactly how much compute power is needed for these models, it's clearly a significant amount. That compute power is provided by datacenters all over the world.

For the rest of this section, I'll be sharing figures of the environmental impact from datacenters. Those figures don't translate exactly to the environmental impact of AI, but it's the closest we can get to understanding that impact, because we don't have exact figures from the companies developing AI tech.

## Water Usage

Computers, obviously, get quite hot when pushed to their limits. For smaller computers like laptops, a couple of fans is generally enough to solve this - the fans can pull cold air in and push hot air out, preventing the system from overheating.

Datacenters, however, are an entirely different beast. They have so many computers and generate so much heat that fans just aren't enough to cool them. Instead, they pull in freshwater, and transfer heat from the datacenter's computers into the water. This has the obvious downside of, well, *using water* - according to a [2023 study from the University of California](https://arxiv.org/pdf/2304.03271), datacenters were responsible for taking between 4.2 and 6.6 billion cubic meters of freshwater for cooling.

Numbers that big don't really make sense without context, so the study also gives us this comparison: Datacenters are using just as much water as half of the entire United Kingdom, every year. Datacenters are using so much water that their usage is comparable to an entire country. Furthermore, that study was done in 2023. I haven't found newer numbers since then, but considering ChatGPT's userbase has doubled since then, water usage is presumably much higher now.


## Energy Usage

Let's switch gears and talk about electricity. Computers use power to run. At smaller scales like laptops and phones, power usage is pretty low, and we don't really think about it. However, at the scale of an entire datacenter, power usage becomes *immense*:

- In 2023, datacenters made up somewhere between [1%-2% of the *entire world's* electricity usage](https://arxiv.org/pdf/2304.03271) (that was 2023, so this number has presumably grown since then).
- A single datacenter can use [just as much power as 50,000 homes](https://thereader.mitpress.mit.edu/the-staggering-ecological-impacts-of-computation-and-the-cloud/).
- According to Google's [2024 environmental report](https://sustainability.google/reports/google-2024-environmental-report/), their carbon emissions have risen by 48% since 2019. There also appears to be a noticeable spike in emissions in 2023, the year they trained and released Gemini:

<img src="data:img/png;base64,#!INCLUDE_BASE64(../assets/google-carbon-emissions.png)" />



# Fleeting Training Data

The final problem with AI that I'll discuss today is its disappearing training data. I'm going to give a brief overview of how LLMs work as some context for this issue.

The GPT of ChatGPT stands for "Generative Pretrained Transformer" - it's a label for the type of AI that ChatGPT and similar LLMs are, and explain how they work. LLMs receive training before they're used. That training configures how the LLM stores and relates different words. After training, the GPT will basically have a database of words stored in a way that puts related words closer together.

> **Note:**
>
> Technically, LLMs work with *tokens*, not words. Tokens are how LLMs break down English, and may be words or pieces of words. For simplicity, I'm still going to call them words.
>
> Here's some examples for ChatGPT, with tokens separated by a "/":
> - "Hi, my name is Bob" is broken down into the tokens "Hi/ my/ name/ is/ Bob"
> - "the orange cat meows" is broken down into the tokens "the/ orange/ cat/ me/ows" - notice that "meows" gets split into 2 tokens
>
> You can test other phrases yourself here: https://platform.openai.com/tokenizer

The GPT is "generative" because it generates content - when you give it a series of words, the GPT will use its database of related words to predict the next word. This is (an oversimplified version of) how ChatGPT and other LLMs work - they just keep generating new words, based on its database of similar words. You can see this in real time when prompt ChatGPT - it's a little hard to see, but the words in its response get generated one after another:

<img src="data:img/gif;base64,#!INCLUDE_BASE64(../assets/gpt-prompt.gif)" />

The important thing to realise here is that the LLM's results depend entirely on its training. The training it receives creates the database that it uses to generate content. Little or bad data will make a bad LLM; lots of very good data will make a great LLM. To keep improving the LLM, you'll need even more, very good data.

> **Note:**
>
> If you want more of an explanation on how LLMs work, 3Blue1Brown has a great series on YouTube about it:
>
> <iframe loading="lazy" width="560" height="315" src="https://www.youtube-nocookie.com/embed/wjZofJX0v4M" title="YouTube video player" frameborder="0"></iframe>

Since AI needs tons of high-quality training data, that data comes from the internet - there's no other source that can provide such vast amounts of training data. The problem is that it appears training data is becoming increasingly scarce, because people are adding restrictions that make it illegal to use their content to train AI. According to a [recent study](https://www.dataprovenance.org/Consent_in_Crisis.pdf), many of the biggest websites that have been used to collect AI training data have restricted as much as 45% of their content (that's nearly *half*), an increase of over 500% from last year. The common sources for new AI training data are becoming less available - and they're becoming less available at a faster rate.

Again, LLMs *only* work because of their training data. Without it, there's no way for them to build the database of words they generate content with. As training data becomes more scarce, LLMs are going to stop improving, and their performance will stagnate.

So when [Sam Altman](https://www.tomsguide.com/ai/chatgpt/sam-altman-claims-agi-is-coming-in-2025-and-machines-will-be-able-to-think-like-humans-when-it-happens) promises we'll have "artificial general intelligence" that thnks like humans in the next few years, it's nothing but marketing. It's a shiny prediction that makes OpenAI sound amazing, which gets the company more investors to stay afloat.

The reality is that long before we have AI as intelligent as humans, or build a Skynet that tries to eradicate us all, we're going to run out of training data to keep improving AI.



# Financial Cost

To me, this is one of the stupid parts of this whole situation: AI, as a product, *is not profitable*. As discussed at the start of this post, AI needs a *lot* of compute power and energy to run. However, most AI products are free, or have an optional subscription. This has the effect of hiding that cost from normal users; you just don't pay for it or see it.

Companies like OpenAI and Anthropic don't publish their financial details publicly. However, they do share financial documents with investors when trying to get new investments. As it turns out, the New York Times managed to get their hands on one of these documents, and they published their analysis of the report publicly. According to [the New York Times' article](https://www.nytimes.com/2024/09/27/technology/openai-chatgpt-investors-funding.html), OpenAI is set to make $3.7 billion this year - but the company will still face a net loss of $5 billion. Furthermore, according to the Times, some expenses are missing from that $5 billion figure:

> Those numbers do not include paying out equity-based compensation to employees, among several large expenses not fully explained in the documents.

This isn't the first year OpenAI has lost money, either. In 2022, while developing ChatGPT, they [reportedly lost $548 million](https://web.archive.org/web/20230619191257/https://www.theinformation.com/articles/openais-losses-doubled-to-540-million-as-it-developed-chatgpt).

Let's put this in a timeline, along with some other data to give full context about OpenAI's growth and finances.

<table>
    <tbody>
    <tr>
        <th>November 2022</th>
        <td>OpenAI releases ChatGPT</td>
    </tr>
    <tr>
        <th>December 2022</th>
        <td>OpenAI loses a reported $548 million</td>
    </tr>
    <tr>
        <th>January 2023</th>
        <td>
            ChatGPT reaches 100 million monthly users,
            becoming the <a href="https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/">fastest growing platform ever</a>
        </td>
    </tr>
    <tr>
        <th>February 2023</th>
        <td>
            OpenAI releases <a href="https://openai.com/index/chatgpt-plus/">ChatGPT Plus</a>,
            a subscription for ChatGPT and source of revenue
        </td>
    </tr>
    <tr>
        <th>November 2023</th>
        <td>OpenAI reaches 100 million weekly users</td>
    </tr>
    <tr>
        <th>August 2024</th>
        <td>
            OpenAI reaches 200 million weekly users,
            doubling its weekly user count from 2023
        </td>
    </tr>
    <tr>
        <th>December 2024</th>
        <td>OpenAI is set to lose $5 billion</td>
    </tr>
    </tbody>
</table>

After releasing a subscription plan, becoming the fastest growing platform ever, then doubling their weekly userbase, OpenAI's yearly losses have grown tenfold.




# What can I do?

So far as I can tell, AI is mostly funded by private investments and some of the wealthiest corporations in existence. It's not some government policy that can be voted against or protested; it's being created through raw economic power.

This economic power feeds on public opinion, and more specifically, misinformation. Here's some examples of that:
- As mentioned before, OpenAI doesn't publicly release financial statements. Because there's no concrete numbers for its finances, all of us in the general public really have to rely on word-of-mouth and gut instinct about how they're doing financially. Then, when they brag about raising billions in investments, and never disclose their losses, it appears as if they're making money.
- In some cases, companies call software "AI" that is actually a completely unrelated technology. The [Rabbit R1](https://www.rabbit.tech/), for example, [raised tens of millions in funding](https://www.rabbit.tech/newsroom/rabbit-raises-additional-10m) for a cheap, handheld AI product. It was later revealed to actually rely on [simple scripts](https://youtu.be/zLvFc_24vSM?t=235) from a well-known project called [Playwright](https://playwright.dev/), and really provided no AI services at all besides ChatGPT queries.

So, then - what can *you* do about these issues with AI? Well, surprisingly, it's quite simple: **talk about what you know**. Knowledge is power, so spread what you've read here, and any additional details you learn in your own research. Since AI is running largely on mislead public opinion, spreading the truth is one of the simplest ways to cut through its bullshit.

Another way you can help fight AI by **not paying for AI products**. When people pay for ChatGPT subscriptions, purchase Rabbit R1 devices, or otherwise pay for AI services, it makes the tech look that much more profitable. That then attracts investors, who can pay to offset the losses from running AI products.

As public opinion slowly shifts away from constant AI hype, investors will follow. And when the investors leave, companies will lose the financial incentive to constantly push new AI products or integrations. All of this is distanced from universities and researchers, who can keep working on this tech, and possibly create something even more amazing in 10 years. But the dangerous part - the public part that uses water, burns power, and constantly requires billions in funding - will fade away.
